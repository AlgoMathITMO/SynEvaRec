{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mysterious-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random as npr\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a3ed26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "SYNTHETC_DATASET_SIZE = 10_000\n",
    "SAMPLE_SIZES = [0.1]\n",
    "ALPHA_SAMPLE_RATE = 2\n",
    "\n",
    "USERPROFILE_DATASET_PATH = \"../../data/restaurants/userprofile.csv\"\n",
    "USERCUISINE_DATASET_PATH = \"../data/restaurants/usercuisine.csv\"\n",
    "CHEFMOZCUISINE_DATASET_PATH = \"../../data/restaurants/chefmozcuisine.csv\"\n",
    "RATINGS_DATASET_PATH = \"../../restaurant_data_reformatted/ratings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-yugoslavia",
   "metadata": {},
   "source": [
    "## Load and transform restaurants data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ac054",
   "metadata": {},
   "source": [
    "Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "global-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_user_profile_df(df):\n",
    "    df = df.drop([\"latitude\", \"longitude\"], axis=1)\n",
    "    df = df.replace(\"?\", pd.NA)\n",
    "    df = df.fillna(method=\"bfill\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_user_cuisine_df(df):\n",
    "    df.drop_duplicates()\n",
    "    df = df.join(pd.get_dummies(df[\"Rcuisine\"]))\n",
    "    df = df.drop(\"Rcuisine\", axis=1)\n",
    "    df = df.groupby(\"userID\").sum()\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_clean_users_df():\n",
    "    user_profile_df = pd.read_csv(USERPROFILE_DATASET_PATH)\n",
    "    user_cuisine_df = pd.read_csv(USERCUISINE_DATASET_PATH)\n",
    "    user_profile_df = prepare_user_profile_df(user_profile_df)\n",
    "    user_cuisine_df = prepare_user_cuisine_df(user_cuisine_df)\n",
    "\n",
    "    users_df = pd.merge(user_profile_df, user_cuisine_df, on=\"userID\")\n",
    "    return users_df\n",
    "\n",
    "\n",
    "def load_and_prepare_rest_cuisine_df():\n",
    "    df = pd.read_csv(CHEFMOZCUISINE_DATASET_PATH)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.join(pd.get_dummies(df[\"Rcuisine\"]))\n",
    "    df = df.drop(\"Rcuisine\", axis=1)\n",
    "    df = df.groupby(\"placeID\").sum()\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abstract-fireplace",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/restaurants/usercuisine.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b01f69609f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0musers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_clean_users_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0musers_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-65978c6ffa93>\u001b[0m in \u001b[0;36mload_and_clean_users_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_clean_users_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0muser_profile_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSERPROFILE_DATASET_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0muser_cuisine_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSERCUISINE_DATASET_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0muser_profile_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_user_profile_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_profile_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0muser_cuisine_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_user_cuisine_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_cuisine_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/university/recsys/.venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/university/recsys/.venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/university/recsys/.venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/university/recsys/.venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/university/recsys/.venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/restaurants/usercuisine.csv'"
     ]
    }
   ],
   "source": [
    "users_df = load_and_clean_users_df()\n",
    "users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "rests_df = load_and_prepare_rest_cuisine_df()\n",
    "rests_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv(RATINGS_DATASET_PATH)\n",
    "ratings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec4609f",
   "metadata": {},
   "source": [
    "Merge ratings dataset, users dataset, and restaurants dataset into single tablem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(ratings_df, users_df, on=\"userID\")\n",
    "merged_df = pd.merge(merged_df, rests_df, on=\"placeID\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-puppy",
   "metadata": {},
   "source": [
    "## Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import GaussianCopula\n",
    "\n",
    "\n",
    "def fit_syn_generator(df):\n",
    "    model = GaussianCopula()\n",
    "    df = df.drop([\"userID\", \"placeID\"], axis=1) # Drop ids\n",
    "    df = df.astype(\"int64\", errors=\"ignore\") # Convert all numbers to int64\n",
    "    model.fit(df.copy())\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.seed(SEED)\n",
    "syn_data_generator = fit_syn_generator(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.seed(SEED)\n",
    "syn_merged_df = syn_data_generator.sample(SYNTHETIC_DATSET_SIZE)\n",
    "syn_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-bangladesh",
   "metadata": {},
   "source": [
    "## Fit DeepFM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "\n",
    "from modules.models import DeepFmModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_feat_names = [\"height\", \"weight\", \"birth_year\"]\n",
    "sparse_feat_names = [\n",
    "    c for c in list(syn_merged_df.columns) if c not in [\n",
    "        \"rating\",\n",
    "        \"food_rating\",\n",
    "        \"service_rating\",\n",
    "        \"weight\",\n",
    "        \"height\",\n",
    "        \"birth_year\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFMDataLoader:\n",
    "    def __init__(self, *, sparse_features, dense_features):\n",
    "        self._sparse_feats = sparse_features\n",
    "        self._dense_feats = dense_features\n",
    "        \n",
    "    def load(self, dataset):\n",
    "        nn_input = pd.DataFrame()\n",
    "        nn_input[self._sparse_feats] = dataset[self._sparse_feats]\n",
    "        nn_input[self._dense_feats] = dataset[self._dense_feats]\n",
    "        \n",
    "        for feat in self._sparse_feats:\n",
    "            encoder = LabelEncoder()\n",
    "            nn_input[feat] = encoder.fit_transform(nn_input[feat])\n",
    "            \n",
    "        mms = MinMaxScaler(feature_range=(0,1))\n",
    "        nn_input[self._dense_feats] = mms.fit_transform(nn_input[self._dense_feats])\n",
    "        \n",
    "        # problems may be here\n",
    "        sparse_feature_columns = [\n",
    "            SparseFeat(feat, vocabulary_size=nn_input[feat].nunique(), embedding_dim=4) \n",
    "            for i, feat in enumerate(self._sparse_feats)\n",
    "        ]\n",
    "\n",
    "        dense_feature_columns = [DenseFeat(feat, 1,) for feat in self._dense_feats]\n",
    "        \n",
    "        dnn_feat_cols = sparse_feature_columns + dense_feature_columns\n",
    "        linear_feat_cols = sparse_feature_columns + dense_feature_columns\n",
    "        \n",
    "        feat_names = get_feature_names(linear_feat_cols + dnn_feat_cols)\n",
    "        return nn_input, dnn_feat_cols, linear_feat_cols, feat_names\n",
    "\n",
    "\n",
    "data_loader = DeepFMDataLoader(sparse_features=sparse_feat_names, dense_features=dense_feat_names)\n",
    "nn_train_input, dnn_feats, lin_feats, feat_names = data_loader.load(syn_merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cross_join(df1, df2):\n",
    "    df1[\"_join_key\"] = 0\n",
    "    df2[\"_join_key\"] = 0\n",
    "    merged_df = df1.merge(df2, on=\"_join_key\")\n",
    "    merged_df = merged_df.drop(\"_join_key\", axis=1)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def nn_prepare_data_for_rating_matrix(users_df, rests_df):\n",
    "    users_df = users_df.drop(\"userID\", axis=1)\n",
    "    users_df[\"user_id\"] = range(0, len(users_df))\n",
    "    rests_df[\"item_id\"] = range(0, len(rests_df))\n",
    "    user_rest_long_table = _cross_join(users_df, rests_df)\n",
    "    return user_rest_long_table\n",
    "    \n",
    "\n",
    "user_rest_long_table = nn_prepare_data_for_rating_matrix(users_df.copy(), rests_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_user_rest_long_table, _dnn_feats, _lin_feats, _feat_names = data_loader.load(user_rest_long_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feats(feats_a, feats_b):\n",
    "    assert len(feats_a) == len(feats_b)\n",
    "    merged = []\n",
    "    for feat_a, feat_b in zip(feats_a, feats_b):\n",
    "        if isinstance(feat_a, DenseFeat):\n",
    "            continue\n",
    "        if feat_a.vocabulary_size >= feat_b.vocabulary_size:\n",
    "            merged.append(feat_a)\n",
    "        else:\n",
    "            merged.append(feat_b)\n",
    "    return merged\n",
    "\n",
    "\n",
    "_merged_feats = merge_feats(dnn_feats, _dnn_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deepfm(feats, feat_names, x, y):\n",
    "    deepfm = DeepFmModel(feats, feats, feat_names)\n",
    "    train_set, test_set = train_test_split(x, test_size=0.2)\n",
    "    deepfm.train(train_set, target_values=y[:len(train_set)])\n",
    "    return deepfm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.seed(SEED)\n",
    "deepfm = train_deepfm(_merged_feats, feat_names, x=nn_train_input, y=syn_merged_df[\"rating\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "class NNModelWrapper:\n",
    "    def __init__(self, trained_nn):\n",
    "        self._nn = trained_nn\n",
    "\n",
    "    def predict_rating_matrix(self, nn_input, merged_df):\n",
    "        y = self._nn.predict(nn_input)\n",
    "        result = pd.DataFrame()\n",
    "        result[\"rating\"] = y.reshape((len(y),))\n",
    "        result[\"user_id\"] = merged_df[\"user_id\"]\n",
    "        result[\"item_id\"] = merged_df[\"item_id\"]\n",
    "        output_matrix = result.pivot(index=\"user_id\", columns=\"item_id\", values=\"rating\")\n",
    "        return output_matrix\n",
    "\n",
    "model_wrapper = NNModelWrapper(deepfm)\n",
    "deepfm_rating_matrix = model_wrapper.predict_rating_matrix(nn_user_rest_long_table, user_rest_long_table)\n",
    "deepfm_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepfm_rating_matrix = np.around(deepfm_rating_matrix)\n",
    "deepfm_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e7ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_deepfm_rating_matrix(syn_df, users_df, rests_df):\n",
    "    dense_feat_names = [\"height\", \"weight\", \"birth_year\"]\n",
    "    sparse_feat_names = [\n",
    "        c for c in list(syn_df.columns) if c not in [\n",
    "            \"rating\",\n",
    "            \"food_rating\",\n",
    "            \"service_rating\",\n",
    "            \"weight\",\n",
    "            \"height\",\n",
    "            \"birth_year\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    data_loader = DeepFMDataLoader(sparse_features=sparse_feat_names, dense_features=dense_feat_names)\n",
    "    nn_train_input, dnn_feats, lin_feats, feat_names = data_loader.load(syn_df)\n",
    "    user_rest_long_table = nn_prepare_data_for_rating_matrix(users_df.copy(), rests_df.copy())\n",
    "    \n",
    "    nn_user_rest_long_table, _dnn_feats, _lin_feats, _feat_names = data_loader.load(user_rest_long_table)\n",
    "    _merged_feats = merge_feats(dnn_feats, _dnn_feats)\n",
    "    deepfm = train_deepfm(_merged_feats, feat_names, x=nn_train_input, y=syn_df[\"rating\"].values)\n",
    "\n",
    "    model_wrapper = NNModelWrapper(deepfm)\n",
    "    deepfm_rating_matrix = model_wrapper.predict_rating_matrix(nn_user_rest_long_table, user_rest_long_table)\n",
    "    return deepfm_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13155c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "_mtx = make_deepfm_rating_matrix(syn_merged_df, users_df, rests_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-blade",
   "metadata": {},
   "source": [
    "## Make similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-usage",
   "metadata": {},
   "source": [
    "Creating a similarity matrix between users and restaurants based on users' food preference and restaurants' cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def make_similarity_matrix(users_df, rests_df):\n",
    "    users_food_pref_df = users_df[rests_df.columns]\n",
    "    return cosine_similarity(users_food_pref_df, rests_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = make_similarity_matrix(users_df, rests_df)\n",
    "sim_matrix = sim_matrix * 2 # Rating is in range [0, 2]\n",
    "sim_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-dialogue",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from modules import models, evaluator, trainers, utils\n",
    "importlib.reload(models)\n",
    "importlib.reload(evaluator)\n",
    "importlib.reload(trainers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFunction:\n",
    "    def __init__(self, heu_matrix, nn_matrix, noise_matrix):\n",
    "        assert heu_matrix.shape == nn_matrix.shape\n",
    "        self._heu_matrix = heu_matrix\n",
    "        self._nn_matrix = nn_matrix\n",
    "        self._noise_matrix = noise_matrix\n",
    "        \n",
    "    def __call__(self, a1: float, a2: float):\n",
    "        a3 = max(0.0, 1 - a1 - a2)\n",
    "        return (\n",
    "            a1 * self._heu_matrix\n",
    "            + a2 * self._nn_matrix\n",
    "            + a3 * npr.normal(0, 1, size=self._heu_matrix.shape)\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6687a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_resp_fn_config = evaluator.ResponseFunctionConfig(\n",
    "    factory=ResponseFunction,\n",
    "    args=[sim_matrix, deepfm_rating_matrix, npr.normal(0, 1, size=sim_matrix.shape)]\n",
    ")\n",
    "\n",
    "_evaluators = [\n",
    "    evaluator.TrainTestExecutorConfig(\n",
    "        factory=trainers.AutoRecTrainTestExecutor,\n",
    "        args={\"config\": {\"epoch\": 50}},\n",
    "        model_name=\"autorec\"\n",
    "    ),\n",
    "    evaluator.TrainTestExecutorConfig(\n",
    "        factory=trainers.SvdTrainTestExecutor,\n",
    "        args={},\n",
    "        model_name=\"svd\"\n",
    "    ),\n",
    "    evaluator.TrainTestExecutorConfig(\n",
    "        factory=trainers.KnnTrainTestExecutor,\n",
    "        args={},\n",
    "        model_name=\"knn\"\n",
    "    )\n",
    "]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "_evaluator = evaluator.Evaluator(_resp_fn_config, n_proc=4)\n",
    "results = _evaluator.evaluate(\n",
    "    _evaluators, \n",
    "    a_sample_rate=ALPHA_SAMPLE_RATE,\n",
    "    test_size=0.1, \n",
    "    sample_sizes=SAMPLE_SIZES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = utils.group_points_by_minimum_error(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66218863",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.rename(\n",
    "    columns={\n",
    "        \"a1\": \"α1\",\n",
    "        \"a2\": \"α2\",\n",
    "        \"rmse\": \"RMSE\",\n",
    "        \"model_name\": \"Model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "results[\"Model\"] = results[\"Model\"].map(\n",
    "    {\n",
    "        \"knn\": \"kNN\",\n",
    "        \"svd\": \"SVD\",\n",
    "        \"autorec\": \"AutoRec\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60614b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import graph_objects as go\n",
    "\n",
    "\n",
    "def visualize_3d_plot(results, sample_sizes):\n",
    "    name_to_color = {\n",
    "        \"kNN\": \"yellow\",\n",
    "        \"SVD\": \"red\",\n",
    "        \"AutoRec\": \"blue\"\n",
    "    }\n",
    "    for sample_size in sample_sizes:\n",
    "        data = results[results[\"sample_size\"] == sample_size]\n",
    "        fig = go.Figure(\n",
    "            layout=go.Layout(\n",
    "                height=600,\n",
    "                width=800,\n",
    "                font=dict(size=16),\n",
    "                title=dict(\n",
    "                    text=f\"Model RMSE depending on α1 and α2 values. \\n{sample_size}\"\n",
    "                ),\n",
    "                margin=dict(l=20, r=20, t=20, b=20),\n",
    "                scene=dict(\n",
    "                    xaxis = dict(title=\"α1\"),\n",
    "                    yaxis = dict(title=\"α2\"),\n",
    "                    zaxis = dict(title=\"RMSE\")\n",
    "                ),\n",
    "                scene_camera=dict(\n",
    "                    up=dict(x=0, y=0, z=1),\n",
    "                    center=dict(x=0, y=0, z=0),\n",
    "                    eye=dict(x=1.2, y=1.8, z=1.0)\n",
    "                )\n",
    "            ),\n",
    "            data=[\n",
    "                go.Scatter3d(\n",
    "                    name=model_name,\n",
    "                    x=data[data[\"Model\"] == model_name][\"α1\"],\n",
    "                    y=data[data[\"Model\"] == model_name][\"α2\"],\n",
    "                    z=data[data[\"Model\"] == model_name][\"RMSE\"],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=name_to_color[model_name],\n",
    "                        line=dict(width=1, color='DarkSlateGrey')\n",
    "                    )\n",
    "                ) for model_name in [\"AutoRec\", \"SVD\", \"kNN\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        fig.show(\"notebook\")\n",
    "        \n",
    "        \n",
    "visualize_3d_plot(results, SAMPLE_SIZES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "fb17c3c70aa2daba89de097746222c240ddefaef361ffa09e744913d93ac8208"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
