{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-operations",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.20.1 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (1.20.1)\n",
      "Requirement already satisfied: sdv==0.8.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: pandas==1.1.4 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (1.1.4)\n",
      "Requirement already satisfied: scipy==1.6.2 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (1.6.2)\n",
      "Collecting deepctr-torch==0.2.6\n",
      "  Using cached deepctr_torch-0.2.6-py3-none-any.whl (63 kB)\n",
      "Collecting scikit-surprise==1.1.1\n",
      "  Using cached scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: torch>=1.1.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from deepctr-torch==0.2.6) (1.8.1)\n",
      "Requirement already satisfied: tqdm in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from deepctr-torch==0.2.6) (4.60.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl (173.9 MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from pandas==1.1.4) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from pandas==1.1.4) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from scikit-surprise==1.1.1) (1.0.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from scikit-surprise==1.1.1) (1.15.0)\n",
      "Requirement already satisfied: rdt<0.5,>=0.4.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sdv==0.8.0) (0.4.1)\n",
      "Requirement already satisfied: ctgan<0.5,>=0.4.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sdv==0.8.0) (0.4.1)\n",
      "Requirement already satisfied: deepecho<0.2,>=0.1.4 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sdv==0.8.0) (0.1.4)\n",
      "Requirement already satisfied: sktime<0.6,>=0.4 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sdv==0.8.0) (0.5.3)\n",
      "Requirement already satisfied: graphviz<1,>=0.13.2 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sdv==0.8.0) (0.16)\n",
      "Requirement already satisfied: Faker<4.15,>=1.0.1 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sdv==0.8.0) (4.14.2)\n",
      "Requirement already satisfied: sdmetrics<0.3,>=0.2.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sdv==0.8.0) (0.2.0)\n",
      "Requirement already satisfied: copulas<0.6,>=0.5.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sdv==0.8.0) (0.5.0)\n",
      "Requirement already satisfied: torchvision<1,>=0.5.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sdv==0.8.0) (0.9.1)\n",
      "Requirement already satisfied: pomegranate<0.14.2,>=0.13.4 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sdv==0.8.0) (0.14.1)\n",
      "Requirement already satisfied: matplotlib<4,>=3.2.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from copulas<0.6,>=0.5.0->sdv==0.8.0) (3.4.1)\n",
      "Requirement already satisfied: scikit-learn<1,>=0.23 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from ctgan<0.5,>=0.4.0->sdv==0.8.0) (0.24.1)\n",
      "Requirement already satisfied: packaging in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from ctgan<0.5,>=0.4.0->sdv==0.8.0) (20.9)\n",
      "Requirement already satisfied: text-unidecode==1.3 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from Faker<4.15,>=1.0.1->sdv==0.8.0) (1.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from matplotlib<4,>=3.2.0->copulas<0.6,>=0.5.0->sdv==0.8.0) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from matplotlib<4,>=3.2.0->copulas<0.6,>=0.5.0->sdv==0.8.0) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from matplotlib<4,>=3.2.0->copulas<0.6,>=0.5.0->sdv==0.8.0) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from matplotlib<4,>=3.2.0->copulas<0.6,>=0.5.0->sdv==0.8.0) (1.3.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from pomegranate<0.14.2,>=0.13.4->sdv==0.8.0) (2.5.1)\n",
      "Requirement already satisfied: pyyaml in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from pomegranate<0.14.2,>=0.13.4->sdv==0.8.0) (5.4.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from networkx>=2.0->pomegranate<0.14.2,>=0.13.4->sdv==0.8.0) (4.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from scikit-learn<1,>=0.23->ctgan<0.5,>=0.4.0->sdv==0.8.0) (2.1.0)\n",
      "Requirement already satisfied: numba>=0.50 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sktime<0.6,>=0.4->sdv==0.8.0) (0.53.1)\n",
      "Requirement already satisfied: statsmodels>=0.12.1 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sktime<0.6,>=0.4->sdv==0.8.0) (0.12.2)\n",
      "Requirement already satisfied: wheel in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from sktime<0.6,>=0.4->sdv==0.8.0) (0.36.2)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from numba>=0.50->sktime<0.6,>=0.4->sdv==0.8.0) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from numba>=0.50->sktime<0.6,>=0.4->sdv==0.8.0) (54.2.0)\n",
      "Requirement already satisfied: patsy>=0.5 in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from statsmodels>=0.12.1->sktime<0.6,>=0.4->sdv==0.8.0) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from torch>=1.1.0->deepctr-torch==0.2.6) (3.7.4.3)\n",
      "Collecting h5py~=2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Using cached tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Using cached tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.4.0-cp38-cp38-macosx_10_11_x86_64.whl (175.5 MB)\n",
      "  Using cached tensorflow-2.3.2-cp38-cp38-macosx_10_11_x86_64.whl (165.2 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.37.0-cp38-cp38-macosx_10_10_x86_64.whl (3.9 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.15.8-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Using cached wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.3.1-cp38-cp38-macosx_10_14_x86_64.whl (165.2 MB)\n",
      "  Using cached tensorflow-2.3.0-cp38-cp38-macosx_10_11_x86_64.whl (165.2 MB)\n",
      "  Using cached tensorflow-2.2.2-cp38-cp38-macosx_10_11_x86_64.whl (175.4 MB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Using cached tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Using cached tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.2.1-cp38-cp38-macosx_10_14_x86_64.whl (175.4 MB)\n",
      "  Using cached tensorflow-2.2.0-cp38-cp38-macosx_10_11_x86_64.whl (175.4 MB)\n",
      "INFO: pip is looking at multiple versions of sklearn to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-5.4-cp38-cp38-macosx_10_9_x86_64.whl (253 kB)\n",
      "  Using cached PyYAML-5.3.1.tar.gz (269 kB)\n",
      "  Using cached PyYAML-5.3.tar.gz (268 kB)\n",
      "  Using cached PyYAML-5.2.tar.gz (265 kB)\n",
      "  Using cached PyYAML-5.1.2.tar.gz (265 kB)\n",
      "  Using cached PyYAML-5.1.1.tar.gz (274 kB)\n",
      "  Using cached PyYAML-5.1.tar.gz (274 kB)\n",
      "  Using cached PyYAML-3.13.tar.gz (270 kB)\n",
      "  Using cached PyYAML-3.12.zip (375 kB)\n",
      "  Using cached PyYAML-3.11.zip (371 kB)\n",
      "  Using cached PyYAML-3.10.zip (364 kB)\n",
      "INFO: pip is looking at multiple versions of sklearn to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of packaging to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting packaging\n",
      "  Using cached packaging-20.8-py2.py3-none-any.whl (39 kB)\n",
      "  Using cached packaging-20.7-py2.py3-none-any.whl (35 kB)\n",
      "  Using cached packaging-20.5-py2.py3-none-any.whl (35 kB)\n",
      "  Using cached packaging-20.4-py2.py3-none-any.whl (37 kB)\n",
      "  Using cached packaging-20.3-py2.py3-none-any.whl (37 kB)\n",
      "  Using cached packaging-20.2-py2.py3-none-any.whl (37 kB)\n",
      "  Using cached packaging-20.1-py2.py3-none-any.whl (36 kB)\n",
      "  Using cached packaging-20.0-py2.py3-none-any.whl (36 kB)\n",
      "  Using cached packaging-19.2-py2.py3-none-any.whl (30 kB)\n",
      "  Using cached packaging-19.1-py2.py3-none-any.whl (30 kB)\n",
      "  Using cached packaging-19.0-py2.py3-none-any.whl (26 kB)\n",
      "  Using cached packaging-18.0-py2.py3-none-any.whl (21 kB)\n",
      "  Using cached packaging-17.1-py2.py3-none-any.whl (24 kB)\n",
      "  Using cached packaging-17.0-py2.py3-none-any.whl (23 kB)\n",
      "  Using cached packaging-16.8-py2.py3-none-any.whl (23 kB)\n",
      "  Using cached packaging-16.7-py2.py3-none-any.whl (22 kB)\n",
      "  Using cached packaging-16.6-py2.py3-none-any.whl (22 kB)\n",
      "  Using cached packaging-16.5-py2.py3-none-any.whl (22 kB)\n",
      "  Using cached packaging-16.4-py2.py3-none-any.whl (22 kB)\n",
      "  Using cached packaging-16.3-py2.py3-none-any.whl (22 kB)\n",
      "  Using cached packaging-16.2-py2.py3-none-any.whl (22 kB)\n",
      "  Using cached packaging-16.1-py2.py3-none-any.whl (21 kB)\n",
      "  Using cached packaging-16.0-py2.py3-none-any.whl (19 kB)\n",
      "  Using cached packaging-15.3-py2.py3-none-any.whl (18 kB)\n",
      "  Using cached packaging-15.2-py2.py3-none-any.whl (18 kB)\n",
      "  Using cached packaging-15.1-py2.py3-none-any.whl (17 kB)\n",
      "  Using cached packaging-15.0-py2.py3-none-any.whl (17 kB)\n",
      "  Using cached packaging-14.5-py2.py3-none-any.whl (16 kB)\n",
      "  Using cached packaging-14.4-py2.py3-none-any.whl (16 kB)\n",
      "  Using cached packaging-14.3-py2.py3-none-any.whl (16 kB)\n",
      "  Using cached packaging-14.2-py2.py3-none-any.whl (13 kB)\n",
      "  Using cached packaging-14.1-py2.py3-none-any.whl (11 kB)\n",
      "  Using cached packaging-14.0-py2.py3-none-any.whl (10 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "INFO: pip is looking at multiple versions of packaging to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: attrs in /Users/vldpro/Workspace/university/recsys/.venv/lib/python3.8/site-packages (from packaging->ctgan<0.5,>=0.4.0->sdv==0.8.0) (20.3.0)\n",
      "INFO: pip is looking at multiple versions of attrs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting attrs\n",
      "  Using cached attrs-20.2.0-py2.py3-none-any.whl (48 kB)\n",
      "  Using cached attrs-20.1.0-py2.py3-none-any.whl (49 kB)\n",
      "  Using cached attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "  Using cached attrs-19.2.0-py2.py3-none-any.whl (40 kB)\n",
      "  Using cached attrs-19.1.0-py2.py3-none-any.whl (35 kB)\n",
      "  Using cached attrs-18.2.0-py2.py3-none-any.whl (34 kB)\n",
      "  Using cached attrs-18.1.0-py2.py3-none-any.whl (28 kB)\n",
      "  Using cached attrs-17.4.0-py2.py3-none-any.whl (31 kB)\n",
      "  Using cached attrs-17.3.0-py2.py3-none-any.whl (29 kB)\n",
      "  Using cached attrs-17.2.0-py2.py3-none-any.whl (24 kB)\n",
      "  Using cached attrs-17.1.0-py2.py3-none-any.whl (26 kB)\n",
      "  Using cached attrs-16.3.0-py2.py3-none-any.whl (21 kB)\n",
      "  Using cached attrs-16.2.0-py2.py3-none-any.whl (19 kB)\n",
      "  Using cached attrs-16.1.0-py2.py3-none-any.whl (19 kB)\n",
      "  Using cached attrs-16.0.0-py2.py3-none-any.whl (15 kB)\n",
      "  Using cached attrs-15.2.0-py2.py3-none-any.whl (14 kB)\n",
      "  Using cached attrs-15.1.0-py2.py3-none-any.whl (14 kB)\n",
      "  Using cached attrs-15.0.0.tar.gz (33 kB)\n",
      "INFO: pip is looking at multiple versions of attrs to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "INFO: pip is looking at multiple versions of tqdm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "  Using cached tqdm-4.58.0-py2.py3-none-any.whl (73 kB)\n",
      "  Using cached tqdm-4.57.0-py2.py3-none-any.whl (72 kB)\n",
      "  Using cached tqdm-4.56.2-py2.py3-none-any.whl (72 kB)\n",
      "  Using cached tqdm-4.56.1-py2.py3-none-any.whl (72 kB)\n",
      "  Using cached tqdm-4.56.0-py2.py3-none-any.whl (72 kB)\n",
      "  Using cached tqdm-4.55.2-py2.py3-none-any.whl (68 kB)\n",
      "  Using cached tqdm-4.55.1-py2.py3-none-any.whl (68 kB)\n",
      "  Using cached tqdm-4.55.0-py2.py3-none-any.whl (68 kB)\n",
      "  Using cached tqdm-4.54.1-py2.py3-none-any.whl (69 kB)\n",
      "  Using cached tqdm-4.54.0-py2.py3-none-any.whl (69 kB)\n",
      "  Using cached tqdm-4.53.0-py2.py3-none-any.whl (70 kB)\n",
      "  Using cached tqdm-4.52.0-py2.py3-none-any.whl (71 kB)\n",
      "  Using cached tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "  Using cached tqdm-4.50.2-py2.py3-none-any.whl (70 kB)\n",
      "  Using cached tqdm-4.50.1-py2.py3-none-any.whl (70 kB)\n",
      "  Using cached tqdm-4.50.0-py2.py3-none-any.whl (70 kB)\n",
      "  Using cached tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "  Using cached tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n",
      "  Using cached tqdm-4.48.1-py2.py3-none-any.whl (68 kB)\n",
      "  Using cached tqdm-4.48.0-py2.py3-none-any.whl (67 kB)\n",
      "  Using cached tqdm-4.47.0-py2.py3-none-any.whl (66 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached tqdm-4.46.1-py2.py3-none-any.whl (63 kB)\n",
      "  Using cached tqdm-4.46.0-py2.py3-none-any.whl (63 kB)\n",
      "  Using cached tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
      "  Using cached tqdm-4.44.1-py2.py3-none-any.whl (60 kB)\n",
      "  Using cached tqdm-4.44.0-py2.py3-none-any.whl (60 kB)\n",
      "  Using cached tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
      "  Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\n",
      "  Using cached tqdm-4.42.0-py2.py3-none-any.whl (59 kB)\n",
      "  Using cached tqdm-4.41.1-py2.py3-none-any.whl (56 kB)\n",
      "  Using cached tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
      "  Using cached tqdm-4.40.2-py2.py3-none-any.whl (55 kB)\n",
      "  Using cached tqdm-4.40.1-py2.py3-none-any.whl (55 kB)\n",
      "  Using cached tqdm-4.40.0-py2.py3-none-any.whl (54 kB)\n",
      "  Using cached tqdm-4.39.0-py2.py3-none-any.whl (53 kB)\n",
      "  Using cached tqdm-4.38.0-py2.py3-none-any.whl (53 kB)\n",
      "  Using cached tqdm-4.37.0-py2.py3-none-any.whl (53 kB)\n",
      "  Using cached tqdm-4.36.1-py2.py3-none-any.whl (52 kB)\n",
      "  Using cached tqdm-4.36.0-py2.py3-none-any.whl (52 kB)\n",
      "  Using cached tqdm-4.35.0-py2.py3-none-any.whl (50 kB)\n",
      "  Using cached tqdm-4.34.0-py2.py3-none-any.whl (50 kB)\n",
      "  Using cached tqdm-4.33.0-py2.py3-none-any.whl (50 kB)\n",
      "  Using cached tqdm-4.32.2-py2.py3-none-any.whl (50 kB)\n",
      "  Using cached tqdm-4.32.1-py2.py3-none-any.whl (49 kB)\n",
      "  Using cached tqdm-4.32.0-py2.py3-none-any.whl (49 kB)\n",
      "  Using cached tqdm-4.31.1-py2.py3-none-any.whl (48 kB)\n",
      "  Using cached tqdm-4.31.0-py2.py3-none-any.whl (48 kB)\n",
      "  Using cached tqdm-4.30.0-py2.py3-none-any.whl (47 kB)\n",
      "  Using cached tqdm-4.29.1-py2.py3-none-any.whl (46 kB)\n",
      "  Using cached tqdm-4.29.0-py2.py3-none-any.whl (46 kB)\n",
      "  Using cached tqdm-4.28.1-py2.py3-none-any.whl (45 kB)\n",
      "  Using cached tqdm-4.28.0-py2.py3-none-any.whl (43 kB)\n",
      "  Using cached tqdm-4.27.0-py2.py3-none-any.whl (44 kB)\n",
      "  Using cached tqdm-4.26.0-py2.py3-none-any.whl (43 kB)\n",
      "  Using cached tqdm-4.25.0-py2.py3-none-any.whl (43 kB)\n",
      "  Using cached tqdm-4.24.0-py2.py3-none-any.whl (43 kB)\n",
      "  Using cached tqdm-4.23.4-py2.py3-none-any.whl (42 kB)\n",
      "  Using cached tqdm-4.23.3-py2.py3-none-any.whl (42 kB)\n",
      "  Using cached tqdm-4.23.2-py2.py3-none-any.whl (42 kB)\n",
      "  Using cached tqdm-4.23.1-py2.py3-none-any.whl (42 kB)\n",
      "  Using cached tqdm-4.23.0-py2.py3-none-any.whl (42 kB)\n",
      "  Using cached tqdm-4.22.0-py2.py3-none-any.whl (42 kB)\n",
      "  Using cached tqdm-4.21.0-py2.py3-none-any.whl (42 kB)\n",
      "  Using cached tqdm-4.20.0-py2.py3-none-any.whl (41 kB)\n",
      "  Using cached tqdm-4.19.9-py2.py3-none-any.whl (52 kB)\n",
      "  Using cached tqdm-4.19.8-py2.py3-none-any.whl (52 kB)\n",
      "  Using cached tqdm-4.19.7-py2.py3-none-any.whl (52 kB)\n",
      "  Using cached tqdm-4.19.6-py2.py3-none-any.whl (52 kB)\n",
      "  Using cached tqdm-4.19.5-py2.py3-none-any.whl (51 kB)\n",
      "  Using cached tqdm-4.19.4-py2.py3-none-any.whl (50 kB)\n",
      "  Using cached tqdm-4.19.2-py2.py3-none-any.whl (50 kB)\n",
      "  Using cached tqdm-4.19.1.post1-py2.py3-none-any.whl (51 kB)\n",
      "  Using cached tqdm-4.19.1-py2.py3-none-any.whl (50 kB)\n",
      "  Using cached tqdm-4.18.0-py2.py3-none-any.whl (48 kB)\n",
      "  Using cached tqdm-4.17.1-py2.py3-none-any.whl (47 kB)\n",
      "  Using cached tqdm-4.17.0-py2.py3-none-any.whl (47 kB)\n",
      "  Using cached tqdm-4.16.0-py2.py3-none-any.whl (47 kB)\n",
      "  Using cached tqdm-4.15.0-py2.py3-none-any.whl (46 kB)\n",
      "  Using cached tqdm-4.14.0-py2.py3-none-any.whl (46 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.20.1 \\\n",
    "             sdv==0.8.0 \\\n",
    "             pandas==1.1.4 \\\n",
    "             scipy==1.6.2 \\\n",
    "             deepctr-torch==0.2.6 \\\n",
    "             scikit-surprise==1.1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import GaussianCopula\n",
    "import numpy as npxm\n",
    "import pandas as pd\n",
    "from numpy import random as npra\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-royalty",
   "metadata": {},
   "source": [
    "## Helper classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFunction:\n",
    "    def __init__(self, heu_matrix, nn_matrix, noise_matrix):\n",
    "        assert heu_matrix.shape == nn_matrix.shape\n",
    "        self._heu_matrix = heu_matrix\n",
    "        self._nn_matrix = nn_matrix\n",
    "        self._noise_matrix = noise_matrix\n",
    "        \n",
    "    def __call__(self, a1: float, a2: float):\n",
    "        a3 = max(0.0, 1 - a1 - a2)\n",
    "        return (\n",
    "            a1 * self._heu_matrix\n",
    "            + a2 * self._nn_matrix\n",
    "            + a3 * npr.normal(0, 1, size=self._heu_matrix.shape)\n",
    "        )\n",
    "    \n",
    "\n",
    "class DeepFMDataLoader:\n",
    "    def __init__(self, *, sparse_features, dense_features):\n",
    "        self._sparse_feats = sparse_features\n",
    "        self._dense_feats = dense_features\n",
    "        \n",
    "    def load(self, dataset):\n",
    "        nn_input = pd.DataFrame()\n",
    "        nn_input[self._sparse_feats] = dataset[self._sparse_feats]\n",
    "        nn_input[self._dense_feats] = dataset[self._dense_feats]\n",
    "        \n",
    "        for feat in self._sparse_feats:\n",
    "            encoder = LabelEncoder()\n",
    "            nn_input[feat] = encoder.fit_transform(nn_input[feat])\n",
    "            \n",
    "        mms = MinMaxScaler(feature_range=(0,1))\n",
    "        nn_input[self._dense_feats] = mms.fit_transform(nn_input[self._dense_feats])\n",
    "        \n",
    "        # problems may be here\n",
    "        sparse_feature_columns = [\n",
    "            SparseFeat(feat, vocabulary_size=nn_input[feat].nunique(), embedding_dim=4) \n",
    "            for i, feat in enumerate(self._sparse_feats)\n",
    "        ]\n",
    "\n",
    "        dense_feature_columns = [DenseFeat(feat, 1,) for feat in self._dense_feats]\n",
    "        \n",
    "        dnn_feat_cols = sparse_feature_columns + dense_feature_columns\n",
    "        linear_feat_cols = sparse_feature_columns + dense_feature_columns\n",
    "        \n",
    "        feat_names = get_feature_names(linear_feat_cols + dnn_feat_cols)\n",
    "        return nn_input, dnn_feat_cols, linear_feat_cols, feat_names\n",
    "      \n",
    "        \n",
    "def merge_feats(feats_a, feats_b):\n",
    "    assert len(feats_a) == len(feats_b)\n",
    "    merged = []\n",
    "    for feat_a, feat_b in zip(feats_a, feats_b):\n",
    "        if isinstance(feat_a, DenseFeat):\n",
    "            continue\n",
    "        if feat_a.vocabulary_size >= feat_b.vocabulary_size:\n",
    "            merged.append(feat_a)\n",
    "        else:\n",
    "            merged.append(feat_b)\n",
    "    return merged\n",
    "            \n",
    "\n",
    "class NNModelWrapper:\n",
    "    def __init__(self, trained_nn):\n",
    "        self._nn = trained_nn\n",
    "\n",
    "    def predict_rating_matrix(self, nn_input, merged_df):\n",
    "        y = self._nn.predict(nn_input)\n",
    "        result = pd.DataFrame()\n",
    "        result[\"rating\"] = y.reshape((len(y),))\n",
    "        result[\"user_id\"] = merged_df[\"user_id\"]\n",
    "        result[\"item_id\"] = merged_df[\"item_id\"]\n",
    "        output_matrix = result.pivot(index=\"user_id\", columns=\"item_id\", values=\"rating\")\n",
    "        return output_matrix\n",
    "    \n",
    "\n",
    "def _cross_join(df1, df2):\n",
    "    df1[\"_join_key\"] = 0\n",
    "    df2[\"_join_key\"] = 0\n",
    "    merged_df = df1.merge(df2, on=\"_join_key\")\n",
    "    merged_df = merged_df.drop(\"_join_key\", axis=1)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def rating_matrix_to_long_table(rating_matrix):\n",
    "    df = pd.DataFrame(rating_matrix)\n",
    "    df[\"user_id\"] = df.index\n",
    "    return df.melt(id_vars=[\"user_id\"], var_name=\"item_id\", value_name=\"rating\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-method",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "- Load and clean the data;\n",
    "- Generate a synthetic dataset;\n",
    "- Fit and evaluate DeepFM model on synthetic data;\n",
    "- Create a simlirity function between between user and item;\n",
    "- Create a rating matrix based on similarity function output;\n",
    "- Create a rating matrix based on DeepFM output;\n",
    "- Fit a SVD model to the matrix generated by `a1 * sim(u, i) + a2 * deepfm(u, i) + a3 * N(0, 1)`;\n",
    "- Display plots;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-transition",
   "metadata": {},
   "source": [
    "# AutoRec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-employer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "technical-yugoslavia",
   "metadata": {},
   "source": [
    "## Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_user_profile_df(df):\n",
    "    df = df.drop([\"latitude\", \"longitude\"], axis=1)\n",
    "    df = df.replace(\"?\", pd.NA)\n",
    "    df = df.fillna(method=\"bfill\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_user_cuisine_df(df):\n",
    "    df.drop_duplicates()\n",
    "    df = df.join(pd.get_dummies(df[\"Rcuisine\"]))\n",
    "    df = df.drop(\"Rcuisine\", axis=1)\n",
    "    df = df.groupby(\"userID\").sum()\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_clean_users_df():\n",
    "    user_profile_df = pd.read_csv(\"../data/restaurant_data/userprofile.csv\")\n",
    "    user_cuisine_df = pd.read_csv(\"../data/restaurant_data/usercuisine.csv\")\n",
    "    user_profile_df = prepare_user_profile_df(user_profile_df)\n",
    "    user_cuisine_df = prepare_user_cuisine_df(user_cuisine_df)\n",
    "\n",
    "    users_df = pd.merge(user_profile_df, user_cuisine_df, on=\"userID\")\n",
    "    return users_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = load_and_clean_users_df()\n",
    "users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_rest_cuisine_df():\n",
    "    df = pd.read_csv(\"../data/restaurant_data/chefmozcuisine.csv\")\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.join(pd.get_dummies(df[\"Rcuisine\"]))\n",
    "    df = df.drop(\"Rcuisine\", axis=1)\n",
    "    df = df.groupby(\"placeID\").sum()\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "rests_df = load_and_prepare_rest_cuisine_df()\n",
    "rests_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv(\"../data/restaurant_data_reformatted/ratings.csv\")\n",
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(ratings_df, users_df, on=\"userID\")\n",
    "merged_df = pd.merge(merged_df, rests_df, on=\"placeID\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-puppy",
   "metadata": {},
   "source": [
    "## Generate syn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_syn_generator(df):\n",
    "    model = GaussianCopula()\n",
    "    df = df.drop([\"userID\", \"placeID\"], axis=1) # Drop ids\n",
    "    df = df.astype(\"int64\", errors=\"ignore\") # Convert all numbers to int64\n",
    "    model.fit(df.copy())\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data_generator = fit_syn_generator(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_merged_df = syn_data_generator.sample(10_000)\n",
    "syn_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-bangladesh",
   "metadata": {},
   "source": [
    "## Fit DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr_torch.models import DeepFM\n",
    "\n",
    "\n",
    "class DeepFmModel:\n",
    "    def __init__(self, linear_feature_columns, dnn_feature_columns, feature_names):\n",
    "        self._linear_feature_columns = linear_feature_columns\n",
    "        self._dnn_feature_columns = dnn_feature_columns\n",
    "        self._feature_names = feature_names\n",
    "        self._deepfm = DeepFM(\n",
    "            self._linear_feature_columns,\n",
    "            self._dnn_feature_columns,\n",
    "            task='multiclass',\n",
    "            device='cpu'\n",
    "        )\n",
    "        self._deepfm.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "        \n",
    "    def train(self, train_set, target_values):\n",
    "        train_model_input = {n: train_set[n] for n in self._feature_names}\n",
    "        history = self._deepfm.fit(\n",
    "            train_model_input,\n",
    "            target_values,\n",
    "            batch_size=256,\n",
    "            epochs=10,\n",
    "            verbose=2,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, test_set):\n",
    "        test_model_input = {n: test_set[n] for n in self._feature_names}\n",
    "        result = self._deepfm.predict(test_model_input, batch_size=256)\n",
    "        return result\n",
    "\n",
    "    #TODO: add evaluate() method\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_feat_names = [\"height\", \"weight\", \"birth_year\"]\n",
    "sparse_feat_names = [\n",
    "    c for c in list(syn_merged_df.columns) if c not in [\n",
    "        \"rating\",\n",
    "        \"food_rating\",\n",
    "        \"service_rating\",\n",
    "        \"weight\",\n",
    "        \"height\",\n",
    "        \"birth_year\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DeepFMDataLoader(sparse_features=sparse_feat_names, dense_features=dense_feat_names)\n",
    "nn_train_input, dnn_feats, lin_feats, feat_names = data_loader.load(syn_merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_prepare_data_for_rating_matrix(users_df, rests_df):\n",
    "    users_df = users_df.drop(\"userID\", axis=1)\n",
    "    users_df[\"user_id\"] = range(0, len(users_df))\n",
    "    rests_df[\"item_id\"] = range(0, len(rests_df))\n",
    "    user_rest_long_table = _cross_join(users_df, rests_df)\n",
    "    return user_rest_long_table\n",
    "\n",
    "user_rest_long_table = nn_prepare_data_for_rating_matrix(users_df.copy(), rests_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_user_rest_long_table, _dnn_feats, _lin_feats, _feat_names = data_loader.load(user_rest_long_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "_merged_feats = merge_feats(dnn_feats, _dnn_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deepfm(feats, feat_names, x, y):\n",
    "    deepfm = DeepFmModel(feats, feats, feat_names)\n",
    "    train_set, test_set = train_test_split(x, test_size=0.2)\n",
    "    deepfm.train(train_set, target_values=y[:len(train_set)])\n",
    "    return deepfm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepfm = train_deepfm(_merged_feats, feat_names, x=nn_train_input, y=syn_merged_df[\"rating\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = NNModelWrapper(deepfm)\n",
    "deepfm_rating_matrix = model_wrapper.predict_rating_matrix(nn_user_rest_long_table, user_rest_long_table)\n",
    "deepfm_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepfm_rating_matrix = np.around(deepfm_rating_matrix)\n",
    "deepfm_rating_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-blade",
   "metadata": {},
   "source": [
    "## Make similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-usage",
   "metadata": {},
   "source": [
    "Creating a similarity matrix between users and restaurants based on users' food preference and restaurants' cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def make_similarity_matrix(users_df, rests_df):\n",
    "    users_food_pref_df = users_df[rests_df.columns]\n",
    "    return cosine_similarity(users_food_pref_df, rests_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = make_similarity_matrix(users_df, rests_df)\n",
    "sim_matrix = sim_matrix * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-dialogue",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import progressbar\n",
    "from sklearn.model_selection import KFold\n",
    "from surprise import Dataset, Reader, SVD, accuracy, KNNBasic\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _svd_train(data, sample_frac):\n",
    "    dataset = Dataset.load_from_df(data[['user_id', 'item_id', 'rating']], Reader(rating_scale=(0, 2)))\n",
    "    train_set, test_set = train_test_split(dataset, test_size=1.0 - sample_frac)\n",
    "    algo = SVD()\n",
    "    algo.fit(train_set)\n",
    "    predictions = algo.test(test_set)\n",
    "    return {\"test_rmse\": [accuracy.rmse(predictions)]}\n",
    "\n",
    "def _knn_train(data, sample_frac):\n",
    "    dataset = Dataset.load_from_df(data[['user_id', 'item_id', 'rating']], Reader(rating_scale=(0, 2)))\n",
    "    train_set, test_set = train_test_split(dataset, test_size=1.0 - sample_frac)\n",
    "    \n",
    "    algo = KNNBasic()\n",
    "    algo.fit(train_set)\n",
    "    predictions = algo.test(test_set)\n",
    "    return {\"test_rmse\": [accuracy.rmse(predictions)]}\n",
    "\n",
    "\n",
    "\n",
    "def _transform_long_table_to_sparse_matrix(self, df, test_size):\n",
    "    n_users = df.user_id.unique().shape[0]\n",
    "    n_items = df.item_id.unique().shape[0]\n",
    "\n",
    "    train_data, test_data = train_test_split(df, test_size=test_size)\n",
    "    train_data = pd.DataFrame(train_data)\n",
    "    test_data = pd.DataFrame(test_data)\n",
    "\n",
    "    train_row = []\n",
    "    train_col = []\n",
    "    train_rating = []\n",
    "\n",
    "    for line in train_data.itertuples():\n",
    "        u = line[1] - 1\n",
    "        i = line[2] - 1\n",
    "        train_row.append(u)\n",
    "        train_col.append(i)\n",
    "        train_rating.append(line[3])\n",
    "    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))\n",
    "\n",
    "    test_row = []\n",
    "    test_col = []\n",
    "    test_rating = []\n",
    "    for line in test_data.itertuples():\n",
    "        test_row.append(line[1] - 1)\n",
    "        test_col.append(line[2] - 1)\n",
    "        test_rating.append(line[3])\n",
    "    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))\n",
    "    print(\"Load data finished. Number of users:\", n_users, \"Number of items:\", n_items)\n",
    "    return train_matrix.todok(), test_matrix.todok(), n_users, n_items\n",
    "\n",
    "\n",
    "def _train_autorec(data, sample_frac):\n",
    "    train_matrix, test_matrix, n_users, n_items = _transform_long_table_to_sparse_matrix(data, test_size=1.0 - sample_frac)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = IAutoRec(sess, n_users, n_items)\n",
    "        model.build_network()\n",
    "        model.execute(train_matrix, test_matrix)\n",
    "    return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "import itertools\n",
    "import collections\n",
    "import functools\n",
    "from multiprocessing import Pool\n",
    "import datetime\n",
    "\n",
    "\n",
    "ResponseFunctionParams = collections.namedtuple(\n",
    "    \"ResponseFunctionParams\",\n",
    "    [\"sim_matrix\", \"deepfm_rating_matrix\", \"noise_matrix\"]\n",
    ")\n",
    "\n",
    "\n",
    "COUNT = 10\n",
    "\n",
    "\n",
    "def _iterate_a2(args, *, train_fn):\n",
    "    a1, sample_frac, resp_fn_params = args\n",
    "    response_function = ResponseFunction(*resp_fn_params)\n",
    "    results = []\n",
    "    a1_normalized = a1 / COUNT\n",
    "    for a2 in range(0, COUNT - a1):\n",
    "        a2_normalized = a2 / COUNT\n",
    "        ground_truth_matrix = response_function(a1_normalized, a2_normalized)\n",
    "        gt_long_table = rating_matrix_to_long_table(ground_truth_matrix)\n",
    "        train_error_log = train_fn(gt_long_table, sample_frac=sample_frac)\n",
    "        \n",
    "        results.append((a1_normalized, a2_normalized, train_error_log))\n",
    "        print(f\"-- Experiment: ({a1_normalized}, {a2_normalized})\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def _experiment(resp_fn_params, *, train_fn, sample_frac=0.5, n_processes=4):\n",
    "    procs_args = [(a1, sample_frac, resp_fn_params) for a1 in range(0, COUNT)]\n",
    "    start_time = datetime.datetime.utcnow()\n",
    "    with Pool(n_processes) as p:\n",
    "        results = p.map(functools.partial(_iterate_a2, train_fn=train_fn), procs_args)\n",
    "    calc_duration = datetime.datetime.utcnow() - start_time\n",
    "    print(f\"Total calcucation duration: {calc_duration}\")\n",
    "    return list(itertools.chain.from_iterable(results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run(train_fn):\n",
    "    noise_matrix = npr.normal(0, 1, size=sim_matrix.shape)\n",
    "    resp_fn_params = ResponseFunctionParams(sim_matrix, deepfm_rating_matrix, noise_matrix)\n",
    "    experiment_results = _experiment(resp_fn_params, train_fn=train_fn)\n",
    "    return experiment_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-parish",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svd_exp_results = _run(_svd_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-plant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_exp_results = _run(_knn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "_results = [(a1, a2, np.mean(res[\"test_rmse\"])) for a1, a2, res in knn_exp_results]\n",
    "knn_results_df = pd.DataFrame(_results, columns=[\"a1\", \"a2\", \"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results_df[\"type\"] = \"knn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "_results = [(a1, a2, np.mean(res[\"test_rmse\"])) for a1, a2, res in svd_exp_results]\n",
    "svd_results_df = pd.DataFrame(_results, columns=[\"a1\", \"a2\", \"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_results_df[\"type\"] = \"svd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([knn_results_df, svd_results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-siemens",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    final_df, \n",
    "    x='a1', \n",
    "    y='a2', \n",
    "    z='rmse', \n",
    "    size_max=8, \n",
    "    opacity=1,\n",
    "    color=\"type\",\n",
    "    color_continuous_scale=px.colors.sequential.thermal[::-1]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "__noise_matrix = npr.normal(0, 1, size=sim_matrix.shape)\n",
    "__resp_fn_params = ResponseFunctionParams(sim_matrix, deepfm_rating_matrix, __noise_matrix)\n",
    "_experiment_results = _experiment(__resp_fn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-picking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-gilbert",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
